{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8632d8-aba3-4ae0-85fa-263c43e76d52",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f5daae-ed5f-4299-bf5b-8b9222be89d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Enter your OpenAI API key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5a410c-d715-4a44-8d20-bc185d8fa2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x735afaa905f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "openai_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31783343-c194-4b3a-9e1d-6b43970aaece",
   "metadata": {},
   "source": [
    "## Downolad FAQ and create index and search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af994c03-75c1-41d3-86b9-ac2196758210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  â€œOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDonâ€™t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download parsed json FAQ\n",
    "import json\n",
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65574dd9-9956-49c2-b1d5-cfe808dda830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x735ae5ea00b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a minsearch index from our FAQ \n",
    "\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb1ff2f-857a-410c-9604-cc25e91a84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran â€˜gradle shadowjarâ€™, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a search function that uses our minsearch index to query FAQ\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# and test it - we get 5 top search results as we hardcoded num_results=5\n",
    "\n",
    "search(\"How to set up Kafka\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd75e4e-c408-44ab-9db7-d158c90aa572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a search_tool - a description for OpenAI SDK to let llm use \n",
    "# our search function - it is called << llm tool use >>\n",
    "# if we don't describe the search function like below then OpenAI would \n",
    "# not be able to use it \n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9648fa4d-2e73-4b1b-913c-6f355eee6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how llm will call our search function:\n",
    "    # f = globals()[f_name]\n",
    "    # result = f(**args) - passing arguments to function which name we got from globals...\n",
    "# it will use a call object and extract search function name \n",
    "# and arguments\n",
    "\n",
    "def make_call(call):\n",
    "    args = json.loads(call.arguments)\n",
    "    f_name = call.name\n",
    "    f = globals()[f_name]\n",
    "    result = f(**args)\n",
    "    result_json = json.dumps(result, indent=2)\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": call.call_id,\n",
    "        \"output\": result_json,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc52d959-d6ba-474c-a7ba-4e9f0150c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[\"search\"] \n",
    "# we should get internal system name of our search function:\n",
    "# <function __main__.search(query)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f3309d0-eadb-4932-aa26-46e8614f418b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and then we can do the pythonic trick - assign function name to a variable and \n",
    "# call it via variable as usual\n",
    "\n",
    "f = globals()[\"search\"]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f563a39-4cfa-44a6-8daf-78791a45c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran â€˜gradle shadowjarâ€™, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the trick - instead of calling \n",
    "# search(\"How to set up Kafka\") we can simply do\n",
    "f(\"How to set up Kafka\") \n",
    "# check with similar search above!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148294b-d8eb-42b0-b553-46153b63f7b7",
   "metadata": {},
   "source": [
    "## RAG internals - hard coded for better visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14093c45-09b6-4d35-861d-971015fc45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM will need this developer prompt to learn how to use our tools\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddd7e36-eebf-4ee7-b3ed-950d0adcead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join it now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0497f42d-a488-46e7-90cd-2fcdc39c1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "# main piece of code is this - \n",
    "# response = openai_client.responses.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         input=chat_messages,\n",
    "#         tools=[search_tool]\n",
    "#     )\n",
    "# - we use openai klient to talk with responses API and pass model name, tools and query + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11dd11b9-d207-4694-a5ba-ed4e4a941415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFunctionToolCall(arguments='{\"query\":\"joining course late enrollment\"}', call_id='call_Ou6kDhAcYxTha6XFT6zhsu6j', name='search', type='function_call', id='fc_68c1a5c185d481a1a3395cdd0fe52cfc0f4dbe2f87c2ed41', status='completed')\n",
      "ResponseFunctionToolCall(arguments='{\"query\":\"enrollment deadlines course data engineering zoomcamp\"}', call_id='call_Kezf7G3nLb0xyDbdCZw0nzMO', name='search', type='function_call', id='fc_68c1a5c3092481a19d78621365759a580f4dbe2f87c2ed41', status='completed')\n",
      "It seems you are interested in joining the \"Data Engineering Zoomcamp\" course. Here's what I've found regarding enrollment and joining the course:\n",
      "\n",
      "1. **Course Start Date**: The course is set to begin on **January 15, 2024**, at 17:00. It's important to register before this date.\n",
      "\n",
      "2. **Registration Process**: You can register using a provided link prior to the course start. Interestingly, even if the registration deadline has passed, if the enrollment form is still open, you may still be able to join.\n",
      "\n",
      "3. **No Confirmation Required**: According to some sources, you might not need strict registration to start learning. You could begin submitting homework and participating without being checked against a registered list.\n",
      "\n",
      "4. **Course Materials Available**: Even after the course finishes, both materials and resources will be available for you to continue learning at your own pace.\n",
      "\n",
      "Would you like more details on how to register, or is there something specific about the course that you're curious about?\n"
     ]
    }
   ],
   "source": [
    "# now let llm decide if it need to search FAQ or can answer immediately\n",
    "# after each loop iteration we increase our context with search results:\n",
    "# - chat_messages.extend(response.output)\n",
    "\n",
    "while True:\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=chat_messages,\n",
    "        tools=[search_tool]\n",
    "    )\n",
    "    \n",
    "    chat_messages.extend(response.output)\n",
    "\n",
    "    has_function_calls = False\n",
    "    \n",
    "    for entry in response.output:\n",
    "        if entry.type == 'message':\n",
    "            print(entry.content[0].text)\n",
    "        if entry.type == 'function_call':\n",
    "            print(entry)\n",
    "            result = make_call(entry)\n",
    "            chat_messages.append(result)\n",
    "            has_function_calls = True\n",
    "\n",
    "    if has_function_calls == False:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e812169-e606-4a64-adf6-8a01a52aa7ab",
   "metadata": {},
   "source": [
    "## Using RAG chat agent class as a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22166c05-6407-49c6-a765-46569b73e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install toyaikit --q\n",
    "\n",
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d926d4-b0b0-402a-a826-656de358a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tools objects\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd41dc6-88a9-4006-a7fd-15798b0fcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5cb763b-80fc-408e-96b8-0734d0f22438",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = DisplayingRunnerCallback(chat_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e400825-acf7-4577-9a32-9255e27fed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_4J6D20mgs0T3LbQPLDdUAH8z', 'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to install kafka on local machine\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to install kafka on local machine\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_qB4RgmaHMa2XbUsLfWfBEiFU', 'output': '[\\n  {\\n    \"text\": \"You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\\\nYou might face some challenges, especially for Windows users. If you face cnd2\\\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\\\nUsing GitHub Codespaces\\\\nSetting up the environment on a cloudV Mcodespace\\\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"How do I check compatibility of local and container Spark versions?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Kafka, you typically follow these general steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Install Java</strong>: Kafka requires Java 8 or a later version. Make sure you have Java installed on your system:</p>\n",
       "<ul>\n",
       "<li>You can download Java from the <a href=\"https://www.oracle.com/java/technologies/javase-jdk11-downloads.html\">official Oracle website</a> or use package managers like <code>apt</code> for Ubuntu (<code>sudo apt install default-jdk</code>).</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Download Kafka</strong>: Go to the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka download page</a> and download the latest version of Kafka.</p>\n",
       "</li>\n",
       "<li><p><strong>Extract the Archive</strong>: After downloading, extract the .tgz or .zip file:</p>\n",
       "<ul>\n",
       "<li>You can use commands like <code>tar -xzf kafka_2.12-&lt;version&gt;.tgz</code> for Linux or use extraction tools on Windows.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Start Kafka</strong>:</p>\n",
       "<ul>\n",
       "<li>Open a terminal/window in the Kafka directory and run the following commands to start ZooKeeper (which Kafka depends on):<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>After ZooKeeper is running, in a new terminal instance, run Kafka:<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Creating a Topic</strong>: Once the broker is running, you can create a Kafka topic:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Producing and Consuming Messages</strong>: You can start a producer and a consumer to send and receive messages:</p>\n",
       "<ul>\n",
       "<li>To start a producer:<pre><code class=\"language-bash\">bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>To start a consumer:<pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you prefer to use Docker, you might want to check Docker commands and use a pre-built Kafka image to set up your environment easily.</p>\n",
       "<h3>Clarifying Question:</h3>\n",
       "<p>Are you planning to set up Kafka on a local machine or using Docker? Is there anything specific you would like to explore further, like configuring Kafka or using it with a specific programming language?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = runner.loop(prompt='how do I install kafka', callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1fd1a06-8a87-4622-a6a9-b8c4de3d73d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install kafka using docker\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install kafka using docker\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_3C4SJ88WDifmocvNzBEOodiS', 'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Kafka using Docker, you can follow these steps:</p>\n",
       "<h3>Step 1: Install Docker</h3>\n",
       "<p>Make sure you have Docker installed on your machine. If you havenâ€™t done that yet, you can download it from the <a href=\"https://www.docker.com/products/docker-desktop\">official Docker website</a>.</p>\n",
       "<h3>Step 2: Create a Docker Compose File</h3>\n",
       "<p>To simplify the setup, you can use Docker Compose. Create a file named <code>docker-compose.yml</code> in your preferred directory with the following content:</p>\n",
       "<pre><code class=\"language-yaml\">version: '2'\n",
       "services:\n",
       "  zookeeper:\n",
       "    image: wurstmeister/zookeeper:3.4.6\n",
       "    ports:\n",
       "      - &quot;2181:2181&quot;\n",
       "\n",
       "  kafka:\n",
       "    image: wurstmeister/kafka:latest\n",
       "    ports:\n",
       "      - &quot;9092:9092&quot;\n",
       "    environment:\n",
       "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
       "      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9094\n",
       "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT\n",
       "      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094\n",
       "    depends_on:\n",
       "      - zookeeper\n",
       "</code></pre>\n",
       "<h3>Step 3: Start Kafka and Zookeeper</h3>\n",
       "<p>Open a terminal in the directory containing your <code>docker-compose.yml</code> file and run:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose up -d\n",
       "</code></pre>\n",
       "<p>This command will start both Zookeeper and Kafka in detached mode.</p>\n",
       "<h3>Step 4: Verify Installation</h3>\n",
       "<p>You can check if the containers are running with:</p>\n",
       "<pre><code class=\"language-bash\">docker ps\n",
       "</code></pre>\n",
       "<h3>Step 5: Creating a Kafka Topic</h3>\n",
       "<p>To create a topic, run the following command in a new terminal window:</p>\n",
       "<pre><code class=\"language-bash\">docker exec -it &lt;kafka-container-id&gt; kafka-topics.sh --create --topic test-topic --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "<p>Replace <code>&lt;kafka-container-id&gt;</code> with the ID or name of your Kafka container from the <code>docker ps</code> output.</p>\n",
       "<h3>Step 6: Producing and Consuming Messages</h3>\n",
       "<ul>\n",
       "<li><strong>Producer</strong>:<pre><code class=\"language-bash\">docker exec -it &lt;kafka-container-id&gt; kafka-console-producer.sh --topic test-topic --bootstrap-server kafka:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><strong>Consumer</strong>:<pre><code class=\"language-bash\">docker exec -it &lt;kafka-container-id&gt; kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server kafka:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h3>Troubleshooting Tips</h3>\n",
       "<ul>\n",
       "<li>If you encounter issues, ensure that Docker is running.</li>\n",
       "<li>Use <code>docker-compose logs</code> to check the logs for any potential errors.</li>\n",
       "</ul>\n",
       "<h3>Clarifying Question:</h3>\n",
       "<p>Do you need help with any specific configurations or commands for Kafka in Docker, such as setting up producers and consumers? Is there anything else you would like to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_messages = runner.loop(\n",
    "    prompt='I want to use docker',\n",
    "    previous_messages=messages,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a7b429-d7f1-4f29-8d7b-f34bc3e5fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How to use DBT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>DBT, or Data Build Tool, is a powerful tool used for transforming data in the data warehouse. It allows users to write SQL queries to create models, run transformations, and manage data effectively. To provide you with a comprehensive answer, I'll look up sections on getting started with DBT, key commands, and best practices.</p>\n",
       "<p>Let me search for detailed information about how to use DBT effectively.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to use DBT getting started comman...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to use DBT getting started commands best practices\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_rp2wk7baeNf1HcVUXsfhHgdQ', 'output': '[\\n  {\\n    \"text\": \"Docker Commands\\\\n# Create a Docker Image from a base image\\\\nDocker run -it ubuntu bash\\\\n#List docker images\\\\nDocker images list\\\\n#List  Running containers\\\\nDocker ps -a\\\\n#List with full container ids\\\\nDocker ps -a --no-trunc\\\\n#Add onto existing image to create new image\\\\nDocker commit -a <User_Name> -m \\\\\"Message\\\\\" container_id New_Image_Name\\\\n# Create a Docker Image with an entrypoint from a base image\\\\nDocker run -it --entry_point=bash python:3.11\\\\n#Attach to a stopped container\\\\nDocker start -ai <Container_Name>\\\\n#Attach to a running container\\\\ndocker exec -it <Container_ID> bash\\\\n#copying from host to container\\\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\\\n#copying from container to host\\\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\\\n#Create an image from a docker file\\\\nDocker build -t <Image_Name> <Location of Dockerfile>\\\\n#DockerFile Options and best practices\\\\nhttps://devopscube.com/build-docker-image/\\\\n#Docker delete all images forcefully\\\\ndocker rmi -f $(docker images -aq)\\\\n#Docker delete all containers forcefully\\\\ndocker rm -f $(docker ps -qa)\\\\n#docker compose creation\\\\nhttps://www.composerize.com/\\\\nGCP Commands\\\\n1.     Create SSH Keys\\\\n2.     Added to the Settings of Compute Engine VM Instance\\\\n3.     SSH-ed into the VM Instance with a config similar to following\\\\nHost my-website.com\\\\nHostName my-website.com\\\\nUser my-user\\\\nIdentityFile ~/.ssh/id_rsa\\\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\\\n5.     Install Docker after\\\\na.     Sudo apt-get update\\\\nb.     Sudo apt-get docker\\\\n6.     To run Docker without SUDO permissions\\\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\\\n7.     Google cloud remote copy\\\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\\\nInstall GCP Cloud SDK on Docker Machine\\\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \\\\\"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\\\\\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\\\nAnaconda Commands\\\\n#Activate environment\\\\nConda Activate <environment_name>\\\\n#DeActivate environment\\\\nConda DeActivate <environment_name>\\\\n#Start iterm without conda environment\\\\nconda config --set auto_activate_base false\\\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\\\nhttps://conda-forge.org/docs/user/introduction.html\\\\nconda --version\\\\nconda update conda\\\\nconda config --add channels conda-forge\\\\nconda config --set channel_priority strict\\\\n#Using Libmamba as Solver\\\\nconda install pgcli  --solver=libmamba\\\\nLinux/MAC Commands\\\\nStarting and Stopping Services on Linux\\\\n\\\\u25cf  \\\\tsudo systemctl start postgresql\\\\n\\\\u25cf  \\\\tsudo systemctl stop postgresql\\\\nStarting and Stopping Services on MAC\\\\n\\\\u25cf      launchctl start postgresql\\\\n\\\\u25cf      launchctl stop postgresql\\\\nIdentifying processes listening to a Port across MAC/Linux\\\\nsudo lsof -i -P -n | grep LISTEN\\\\n$ sudo netstat -tulpn | grep LISTEN\\\\n$ sudo ss -tulpn | grep LISTEN\\\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\\\n$ sudo nmap -sTU -O IP-address-Here\\\\nInstalling a package on Debian\\\\nsudo apt install <packagename>\\\\nListing all package on Debian\\\\nDpkg -l | grep <packagename>\\\\nUnInstalling a package on Debian\\\\nSudo apt remove <packagename>\\\\nSudo apt autoclean  && sudo apt autoremove\\\\nList all Processes on Debian/Ubuntu\\\\nPs -aux\\\\napt-get update && apt-get install procps\\\\napt-get install iproute2 for ss -tulpn\\\\n#Postgres Install\\\\nsudo sh -c \\'echo \\\\\"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\\\\\" > /etc/apt/sources.list.d/pgdg.list\\'\\\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\\\nsudo apt-get update\\\\nsudo apt-get -y install postgresql\\\\n#Changing Postgresql port to 5432\\\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\\\n- sudo chown postgres postgresql.conf\\\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\\\n- sudo systemctl restart postgresql\",\\n    \"section\": \"Triggers in Mage via CLI\",\\n    \"question\": \"Basic Commands\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"More info in the Docker Docs on Best Practises\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You need to create packages.yml file in main project directory and add packages\\\\u2019 meta data:\\\\npackages:\\\\n- package: dbt-labs/dbt_utils\\\\nversion: 0.8.0\\\\nAfter creating file run:\\\\nAnd hit enter.\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"When You are getting error dbt_utils not found\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Update the seed column types in the dbt_project.yaml file\\\\nfor using double : float\\\\nfor using int : numeric\\\\nDBT Cloud production error: prod dataset not available in location EU\\\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \\\\\"ERROR 404: porject.dataset:prod not available in location EU\\\\\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"If you are getting not found in location us error.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\\\nAnswer: when you create the CI/CD job, under \\\\u2018Compare Changes against an environment (Deferral) make sure that you select \\\\u2018 No; do not defer to another environment\\\\u2019 - otherwise dbt won\\\\u2019t merge your dev models into production models; it will create a new environment called \\\\u2018dbt_cloud_pr_number of pull request\\\\u2019\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"using DBT tutorials examples commands...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"using DBT tutorials examples commands workflow\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_LczsFXRSZbipDJVyoOU0bKaZ', 'output': '[\\n  {\\n    \"text\": \"1. Go to your dbt cloud service account\\\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"When executing dbt run after using fhv_tripdata as an external table: you get \\\\u201cAccess Denied: BigQuery BigQuery: Permission denied\\\\u201d\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"ImportError: DLL load failed while importing cimpl: The specified module could not be found\\\\nVerify Python Version:\\\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\\\nfrom ctypes import CDLL\\\\nCDLL(\\\\\"C:\\\\\\\\\\\\\\\\Users\\\\\\\\\\\\\\\\YOUR_USER_NAME\\\\\\\\\\\\\\\\anaconda3\\\\\\\\\\\\\\\\envs\\\\\\\\\\\\\\\\dtcde\\\\\\\\\\\\\\\\Lib\\\\\\\\\\\\\\\\site-packages\\\\\\\\\\\\\\\\confluent_kafka.libs\\\\\\\\librdkafka-5d2e2910.dll\\\\\")\\\\nIt seems that the error may occur depending on the OS and python version installed.\\\\nALTERNATIVE:\\\\nImportError: DLL load failed while importing cimpl\\\\n\\\\u2705SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\\\nYou need to set this DLL manually in Conda Env.\\\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Error importing cimpl dll when running avro examples\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Docker Commands\\\\n# Create a Docker Image from a base image\\\\nDocker run -it ubuntu bash\\\\n#List docker images\\\\nDocker images list\\\\n#List  Running containers\\\\nDocker ps -a\\\\n#List with full container ids\\\\nDocker ps -a --no-trunc\\\\n#Add onto existing image to create new image\\\\nDocker commit -a <User_Name> -m \\\\\"Message\\\\\" container_id New_Image_Name\\\\n# Create a Docker Image with an entrypoint from a base image\\\\nDocker run -it --entry_point=bash python:3.11\\\\n#Attach to a stopped container\\\\nDocker start -ai <Container_Name>\\\\n#Attach to a running container\\\\ndocker exec -it <Container_ID> bash\\\\n#copying from host to container\\\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\\\n#copying from container to host\\\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\\\n#Create an image from a docker file\\\\nDocker build -t <Image_Name> <Location of Dockerfile>\\\\n#DockerFile Options and best practices\\\\nhttps://devopscube.com/build-docker-image/\\\\n#Docker delete all images forcefully\\\\ndocker rmi -f $(docker images -aq)\\\\n#Docker delete all containers forcefully\\\\ndocker rm -f $(docker ps -qa)\\\\n#docker compose creation\\\\nhttps://www.composerize.com/\\\\nGCP Commands\\\\n1.     Create SSH Keys\\\\n2.     Added to the Settings of Compute Engine VM Instance\\\\n3.     SSH-ed into the VM Instance with a config similar to following\\\\nHost my-website.com\\\\nHostName my-website.com\\\\nUser my-user\\\\nIdentityFile ~/.ssh/id_rsa\\\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\\\n5.     Install Docker after\\\\na.     Sudo apt-get update\\\\nb.     Sudo apt-get docker\\\\n6.     To run Docker without SUDO permissions\\\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\\\n7.     Google cloud remote copy\\\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\\\nInstall GCP Cloud SDK on Docker Machine\\\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \\\\\"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\\\\\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\\\nAnaconda Commands\\\\n#Activate environment\\\\nConda Activate <environment_name>\\\\n#DeActivate environment\\\\nConda DeActivate <environment_name>\\\\n#Start iterm without conda environment\\\\nconda config --set auto_activate_base false\\\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\\\nhttps://conda-forge.org/docs/user/introduction.html\\\\nconda --version\\\\nconda update conda\\\\nconda config --add channels conda-forge\\\\nconda config --set channel_priority strict\\\\n#Using Libmamba as Solver\\\\nconda install pgcli  --solver=libmamba\\\\nLinux/MAC Commands\\\\nStarting and Stopping Services on Linux\\\\n\\\\u25cf  \\\\tsudo systemctl start postgresql\\\\n\\\\u25cf  \\\\tsudo systemctl stop postgresql\\\\nStarting and Stopping Services on MAC\\\\n\\\\u25cf      launchctl start postgresql\\\\n\\\\u25cf      launchctl stop postgresql\\\\nIdentifying processes listening to a Port across MAC/Linux\\\\nsudo lsof -i -P -n | grep LISTEN\\\\n$ sudo netstat -tulpn | grep LISTEN\\\\n$ sudo ss -tulpn | grep LISTEN\\\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\\\n$ sudo nmap -sTU -O IP-address-Here\\\\nInstalling a package on Debian\\\\nsudo apt install <packagename>\\\\nListing all package on Debian\\\\nDpkg -l | grep <packagename>\\\\nUnInstalling a package on Debian\\\\nSudo apt remove <packagename>\\\\nSudo apt autoclean  && sudo apt autoremove\\\\nList all Processes on Debian/Ubuntu\\\\nPs -aux\\\\napt-get update && apt-get install procps\\\\napt-get install iproute2 for ss -tulpn\\\\n#Postgres Install\\\\nsudo sh -c \\'echo \\\\\"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\\\\\" > /etc/apt/sources.list.d/pgdg.list\\'\\\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\\\nsudo apt-get update\\\\nsudo apt-get -y install postgresql\\\\n#Changing Postgresql port to 5432\\\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\\\n- sudo chown postgres postgresql.conf\\\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\\\n- sudo systemctl restart postgresql\",\\n    \"section\": \"Triggers in Mage via CLI\",\\n    \"question\": \"Basic Commands\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Install the astronomer-cosmos package as a dependency. (see Terraform example).\\\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\\\nYour dbt lineage graph should now appear as tasks inside a task group like this:\",\\n    \"section\": \"Course Management Form for Homeworks\",\\n    \"question\": \"How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Issue: If you don\\\\u2019t define the column format while converting from csv to parquet Python will \\\\u201cchoose\\\\u201d based on the first rows.\\\\n\\\\u2705Solution: Defined the schema while running web_to_gcp.py pipeline.\\\\nSebastian adapted the script:\\\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\\\nNeed a quick change to make the file work with gz files, added the following lines (and don\\\\u2019t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\\\nfile_name_gz = f\\\\\"{service}_tripdata_{year}-{month}.csv.gz\\\\\"\\\\nopen(file_name_gz, \\'wb\\').write(r.content)\\\\nos.system(f\\\\\"gzip -d {file_name_gz}\\\\\")\\\\nos.system(f\\\\\"rm {file_name_init}.*\\\\\")\\\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\\\n\\\\u201cParquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64\\\\u201d\\\\n\\\\u5f00\\\\u542f\\\\u5c4f\\\\u5e55\\\\u9605\\\\u8bfb\\\\u5668\\\\u652f\\\\u6301\\\\n\\\\u8981\\\\u542f\\\\u7528\\\\u5c4f\\\\u5e55\\\\u9605\\\\u8bfb\\\\u5668\\\\u652f\\\\u6301\\\\uff0c\\\\u8bf7\\\\u6309Ctrl+Alt+Z\\\\u3002\\\\u8981\\\\u4e86\\\\u89e3\\\\u952e\\\\u76d8\\\\u5feb\\\\u6377\\\\u952e\\\\uff0c\\\\u8bf7\\\\u6309Ctrl+\\\\u659c\\\\u6760\\\\u3002\\\\n\\\\u67e5\\\\u627e\\\\u548c\\\\u66ff\\\\u6362\\\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\\\nThere are some possible fixes:\\\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\\\nSELECT * EXCEPT (ehail_fee) FROM\\\\u2026\\\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\\\nModify Airflow dag to make the conversion and avoid the error.\\\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\\\nHere is another possibility that could be interesting:\\\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\\\npd.from_csv(..., dtype=type_dict)\\\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital \\\\u2018I\\\\u2019. The type_dict is a python dictionary mapping the column names to the dtypes.\\\\nSources:\\\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\\\nNullable integer data type \\\\u2014 pandas 1.5.3 documentation\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - I am having problems with columns datatype while running DBT/BigQuery\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To effectively use DBT (Data Build Tool), here's a comprehensive overview based on various aspects, including getting started, key commands, and best practices:</p>\n",
       "<h3>1. Getting Started with DBT</h3>\n",
       "<ul>\n",
       "<li><p><strong>Installation</strong>: First, ensure you have Python and pip installed. You can install DBT using pip:</p>\n",
       "<pre><code class=\"language-bash\">pip install dbt\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Creating a DBT Project</strong>: To start a new DBT project:</p>\n",
       "<pre><code class=\"language-bash\">dbt init my_project\n",
       "</code></pre>\n",
       "<p>Replace <code>my_project</code> with your desired project name.</p>\n",
       "</li>\n",
       "</ul>\n",
       "<h3>2. Basic Commands</h3>\n",
       "<ul>\n",
       "<li><strong>Running Models</strong>: To execute the transformations defined in your models, use:<pre><code class=\"language-bash\">dbt run\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><strong>Testing Models</strong>: To test the integrity and assumptions in your models, run:<pre><code class=\"language-bash\">dbt test\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><strong>Building Documentation</strong>: Generate documentation for your project:<pre><code class=\"language-bash\">dbt docs generate\n",
       "dbt docs serve\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h3>3. Configuring Your DBT Project</h3>\n",
       "<ul>\n",
       "<li><strong>Profiles.yml</strong>: This file is essential for defining connection details to your database. Make sure to set it up correctly in the <code>~/.dbt/</code> directory.</li>\n",
       "</ul>\n",
       "<h3>4. Best Practices</h3>\n",
       "<ul>\n",
       "<li><strong>Modular Models</strong>: Break your transformations into smaller, reusable models.</li>\n",
       "<li><strong>Version Control</strong>: Use Git to version control your DBT project.</li>\n",
       "<li><strong>Documentation</strong>: Regularly document your models for better understanding.</li>\n",
       "</ul>\n",
       "<h3>5. Environment Setup</h3>\n",
       "<ul>\n",
       "<li><strong>Docker</strong>: Optionally, you can run DBT in a Docker container for an isolated environment. Create a Dockerfile for your DBT project.</li>\n",
       "</ul>\n",
       "<h3>Example Workflow</h3>\n",
       "<ol>\n",
       "<li><strong>Initialize your project</strong>:<pre><code class=\"language-bash\">dbt init your_project\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><strong>Create your models in the models directory</strong>.</li>\n",
       "<li><strong>Run DBT to build the models</strong>:<pre><code class=\"language-bash\">dbt run\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Please let me know if there are specific areas you would like to explore further, such as more advanced DBT features, troubleshooting common errors, or integrating DBT with other tools.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# finally full search agent with interface\n",
    "# type STOP to exit\n",
    "\n",
    "messages = runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b71621-555f-4178-a6e2-d68fa4fe0d41",
   "metadata": {},
   "source": [
    "## RAG - LLM tool use - adding more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a329a3-91fc-4817-86db-c196d49d4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will have 2 tools - search and add entry\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def search(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search the FAQ database for entries matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query text to look up in the course FAQ.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "    \"\"\"\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def add_entry(question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new entry to the FAQ database.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be added to the FAQ database.\n",
    "        answer (str): The corresponding answer to the question.\n",
    "    \"\"\"\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef19699e-51e3-4b86-b841-d5912a97eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets actually add it:\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search)\n",
    "agent_tools.add_tool(add_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea083c49-c215-4e8d-8b6b-11ddbc376fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'question parameter'},\n",
       "    'answer': {'type': 'string', 'description': 'answer parameter'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and check that both tools are added:\n",
    "\n",
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20243be1-28ae-45ad-b43c-8be7de5fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our agent loop wrapper \n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0af44222-26b9-4447-9c62-c11fdfe372e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 success tips\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"module 1 success tips\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"module 1 success tips\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_KyATltCMn6II92Sissc616sm', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 112\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 124\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"module 1 tips for success in data eng...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"module 1 tips for success in data engineering\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_1kMgnJ3psImmqTQy9kVWGsaj', 'output': '[\\n  {\\n    \"text\": \"Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 377\\n  },\\n  {\\n    \"text\": \"Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\\\nMove the contents of the .gitignore file in your main .gitignore.\\\\nUse the terminal to cd into the Mage folder and:\\\\nrun \\\\u201cgit remote remove origin\\\\u201d to de-couple the Mage repo,\\\\nrun \\\\u201crm -rf .git\\\\u201d to delete local git files,\\\\nrun \\\\u201cgit add .\\\\u201d to add the current folder as changes to stage, commit and push.\",\\n    \"section\": \"Module 2: Workflow Orchestration\",\\n    \"question\": \"Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 191\\n  },\\n  {\\n    \"text\": \"This error appeared when running the command: docker build -t taxi_ingest:v001 .\\\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn\\\\u2019t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\\\nsudo chown -R $USER dir_path\\\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\\\n\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\tAdded by\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\tKenan Arslanbay\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Docker - build error: error checking context: \\'can\\'t stat \\'/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data\\'\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 67\\n  },\\n  {\\n    \"text\": \"You don\\'t need it. You\\'re accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 3\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems that there isn't a dedicated resource titled &quot;Module 1 Success Tips&quot; available in the database. However, I found some relevant information about Module 1, which covers Docker and Terraform, as well as common challenges faced in that module. Here are some points that can help you succeed:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand Docker Basics</strong>: Familiarize yourself with the Docker environment. Understanding how to build Docker images and manage containers is crucial.</p>\n",
       "</li>\n",
       "<li><p><strong>Follow Instructions Carefully</strong>: Make sure to follow instructions as outlined in the course materials, especially when running commands, as minor errors can lead to issues. For example, ensure you're in the correct directory when executing Docker commands.</p>\n",
       "</li>\n",
       "<li><p><strong>Check Permissions</strong>: When encountering errors related to accessing directories or files (such as permission denied errors), remember to check the permissions of the directories you're working in. You might need to modify permissions on Linux systems using commands like <code>sudo chown</code>.</p>\n",
       "</li>\n",
       "<li><p><strong>Use the Right Commands</strong>: Pay attention to the commands being used. For example, make sure you adapt any command needed to build and run your Docker containers correctly.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with the Community</strong>: Utilize the course forums or discussion groups to ask questions and learn from others' experiences. They can offer valuable insights into troubleshooting common issues you might face in this module.</p>\n",
       "</li>\n",
       "<li><p><strong>Practice Regularly</strong>: Regular practice in deploying with Docker and managing containers will help reinforce your understanding and skills.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you would like more specific advice or have questions on particular areas of Module 1, feel free to ask! Are there other topics or challenges in the course that you would like to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# and run it - type STOP as usual to exit:\n",
    "\n",
    "runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023227c2-c9e7-4550-a1ed-46e67379b346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

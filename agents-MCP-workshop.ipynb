{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8632d8-aba3-4ae0-85fa-263c43e76d52",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f5daae-ed5f-4299-bf5b-8b9222be89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5a410c-d715-4a44-8d20-bc185d8fa2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x75c6abecd790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "openai_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31783343-c194-4b3a-9e1d-6b43970aaece",
   "metadata": {},
   "source": [
    "## Downolad FAQ and create index and search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af994c03-75c1-41d3-86b9-ac2196758210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  â€œOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDonâ€™t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download parsed json FAQ\n",
    "import json\n",
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65574dd9-9956-49c2-b1d5-cfe808dda830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x75c6a92334a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a minsearch index from our FAQ \n",
    "\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb1ff2f-857a-410c-9604-cc25e91a84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran â€˜gradle shadowjarâ€™, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a search function that uses our minsearch index to query FAQ\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# and test it - we get 5 top search results as we hardcoded num_results=5\n",
    "\n",
    "search(\"How to set up Kafka\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd75e4e-c408-44ab-9db7-d158c90aa572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a search_tool - a description for OpenAI SDK to let llm use \n",
    "# our search function - it is called << llm tool use >>\n",
    "# if we don't describe the search function like below then OpenAI would \n",
    "# not be able to use it \n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9648fa4d-2e73-4b1b-913c-6f355eee6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how llm will call our search function:\n",
    "    # f = globals()[f_name]\n",
    "    # result = f(**args) - passing arguments to function which name we got from globals...\n",
    "# it will use a call object and extract search function name \n",
    "# and arguments\n",
    "\n",
    "def make_call(call):\n",
    "    args = json.loads(call.arguments)\n",
    "    f_name = call.name\n",
    "    f = globals()[f_name]\n",
    "    result = f(**args)\n",
    "    result_json = json.dumps(result, indent=2)\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": call.call_id,\n",
    "        \"output\": result_json,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc52d959-d6ba-474c-a7ba-4e9f0150c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[\"search\"] \n",
    "# we should get internal system name of our search function:\n",
    "# <function __main__.search(query)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3309d0-eadb-4932-aa26-46e8614f418b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and then we can do the pythonic trick - assign function name to a variable and \n",
    "# call it via variable as usual\n",
    "\n",
    "f = globals()[\"search\"]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f563a39-4cfa-44a6-8daf-78791a45c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran â€˜gradle shadowjarâ€™, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the trick - instead of calling \n",
    "# search(\"How to set up Kafka\") we can simply do\n",
    "f(\"How to set up Kafka\") \n",
    "# check with similar search above!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148294b-d8eb-42b0-b553-46153b63f7b7",
   "metadata": {},
   "source": [
    "## RAG internals - hard coded for better visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14093c45-09b6-4d35-861d-971015fc45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM will need this developer prompt to learn how to use our tools\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cddd7e36-eebf-4ee7-b3ed-950d0adcead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join it now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0497f42d-a488-46e7-90cd-2fcdc39c1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "# main piece of code is this - \n",
    "# response = openai_client.responses.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         input=chat_messages,\n",
    "#         tools=[search_tool]\n",
    "#     )\n",
    "# - we use openai klient to talk with responses API and pass model name, tools and query + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11dd11b9-d207-4694-a5ba-ed4e4a941415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFunctionToolCall(arguments='{\"query\":\"join course late enrollment\"}', call_id='call_HehINlpgVX9weBOFfjIjTb71', name='search', type='function_call', id='fc_68c1be65f55c81a0b3671035717eead909140a3903ee0d2c', status='completed')\n",
      "ResponseFunctionToolCall(arguments='{\"query\":\"course join after start date late enrollment eligibility\"}', call_id='call_4MR5ZnRZCSFdHdDuNWqStJiB', name='search', type='function_call', id='fc_68c1be670aa881a09bdce4704849e89709140a3903ee0d2c', status='completed')\n",
      "Yes, you can still join the course even after it has started! While official registration may close, you're eligible to engage with the materials and submit homework assignments without registering. Just keep in mind that there will be deadlines for the final projects, so try not to wait until the last minute to complete them.\n",
      "\n",
      "Additionally, it may be helpful to familiarize yourself with the course materials and participate in any available office hours or discussions.\n",
      "\n",
      "Would you like information on any specific aspects of the course, such as project deadlines or resources?\n"
     ]
    }
   ],
   "source": [
    "# now let llm decide if it need to search FAQ or can answer immediately\n",
    "# after each loop iteration we increase our context with search results:\n",
    "# - chat_messages.extend(response.output)\n",
    "\n",
    "while True:\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=chat_messages,\n",
    "        tools=[search_tool]\n",
    "    )\n",
    "    \n",
    "    chat_messages.extend(response.output)\n",
    "\n",
    "    has_function_calls = False\n",
    "    \n",
    "    for entry in response.output:\n",
    "        if entry.type == 'message':\n",
    "            print(entry.content[0].text)\n",
    "        if entry.type == 'function_call':\n",
    "            print(entry)\n",
    "            result = make_call(entry)\n",
    "            chat_messages.append(result)\n",
    "            has_function_calls = True\n",
    "\n",
    "    if has_function_calls == False:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e812169-e606-4a64-adf6-8a01a52aa7ab",
   "metadata": {},
   "source": [
    "## Using RAG chat agent class as a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22166c05-6407-49c6-a765-46569b73e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install toyaikit --q\n",
    "\n",
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d926d4-b0b0-402a-a826-656de358a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tools objects\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd41dc6-88a9-4006-a7fd-15798b0fcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat interface makes output prettier - \n",
    "# as it is actually creating markdown responses which look much better than plain text above \n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "# runner object instantiates our while loop for chat agent like we did above\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5cb763b-80fc-408e-96b8-0734d0f22438",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = DisplayingRunnerCallback(chat_interface)\n",
    "# to display clickable black triangles for debugging to see what is inside calls to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e400825-acf7-4577-9a32-9255e27fed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To provide you with a comprehensive answer on how to install Kafka, I'll first search for installation instructions and best practices. I'll look into various methods, such as installing on different operating systems and any prerequisites that may be needed.</p>\n",
       "<p>Let me gather that information for you.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Kafka instructions\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Kafka instructions\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_xvoYWo5U1p1v1dElRGtmSpwF', 'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Kafka installation on Linux Windows M...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"Kafka installation on Linux Windows MacOS\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_vnUHDAui9JsHXu0SdafMVR2t', 'output': '[\\n  {\\n    \"text\": \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"terraformRun this command before starting your VM:\\\\nOn Intel CPU:\\\\nmodprobe -r kvm_intel\\\\nmodprobe kvm_intel nested=1\\\\nOn AMD CPU:\\\\nmodprobe -r kvm_amd\\\\nmodprobe kvm_amd nested=1\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\\\nInstall OpenJDK 11,\\\\non MacOS: $ brew install java11\\\\nAdd export PATH=\\\\\"/opt/homebrew/opt/openjdk@11/bin:$PATH\\\\\"\\\\nto ~/.bashrc or ~/zshrc\\\\nActivate working environment (by pipenv / poetry / conda)\\\\nRun $ pip install pyspark\\\\nWork with exercises as normal\\\\nAll default commands of spark will be also available at shell session under activated enviroment.\\\\nHope this can help!\\\\nP.s. you won\\\\u2019t need findspark to firstly initialize.\\\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\\\n\\\\n$ sdk install java 17.0.10-librca\\\\n$ sdk install spark 3.5.0\\\\n$ sdk install hadoop 3.3.5\\\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\\\n\\\\n\\\\n$ conda create -n ENV_NAME python=3.11\\\\n$ conda activate ENV_NAME\\\\n$ pip install pyspark==3.5.0\\\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\\\nHere my\\\\nexport JAVA_HOME=\\\\\"/c/tools/jdk-11.0.21\\\\\"\\\\nexport PATH=\\\\\"${JAVA_HOME}/bin:${PATH}\\\\\"\\\\nexport HADOOP_HOME=\\\\\"/c/tools/hadoop-3.2.0\\\\\"\\\\nexport PATH=\\\\\"${HADOOP_HOME}/bin:${PATH}\\\\\"\\\\nexport SPARK_HOME=\\\\\"/c/tools/spark-3.3.2-bin-hadoop3\\\\\"\\\\nexport PATH=\\\\\"${SPARK_HOME}/bin:${PATH}\\\\\"\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}/python/:$PYTHONPATH\\\\\"\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\\\\\"\\\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Java+Spark - Easy setup with miniconda env (worked on MacOS)\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you\\\\u2019re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\\\nwslview index.html\\\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\\\nexport BROWSER=\\'/mnt/c/Program Files/Firefox/firefox.exe\\'\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Opening an HTML file with a Windows browser from Linux running on WSL\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here's a guide on how to install Apache Kafka, which includes steps for different operating systems:</p>\n",
       "<h3>Prerequisites</h3>\n",
       "<ol>\n",
       "<li><strong>Java</strong>: Kafka requires Java (at least JDK 8). Make sure you have it installed.<ul>\n",
       "<li>You can check your Java installation with:<pre><code>java -version\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>Install Java if it's not already installed:<ul>\n",
       "<li><strong>On Windows/Mac</strong>: Download and install from the <a href=\"https://adoptopenjdk.net/\">Oracle website or AdoptOpenJDK</a>.</li>\n",
       "<li><strong>On Linux</strong>: Use a package manager, e.g., <code>sudo apt install openjdk-11-jdk</code> for Ubuntu.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Installation Steps</h3>\n",
       "<h4>1. Download Kafka</h4>\n",
       "<ul>\n",
       "<li>Go to the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka download page</a> and download the latest version of Kafka (choose the binary downloads).</li>\n",
       "</ul>\n",
       "<h4>2. Extract the Downloaded Archive</h4>\n",
       "<ul>\n",
       "<li><strong>On Windows</strong>: Use a tool like WinRAR or 7-Zip to extract the <code>.tgz</code> file.</li>\n",
       "<li><strong>On Linux/Mac</strong>: Run:<pre><code class=\"language-sh\">tar -xzf kafka_2.12-*.*.tgz\n",
       "cd kafka_2.12-*.*  # Navigate to the extracted folder\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h4>3. Start the Kafka Environment</h4>\n",
       "<ul>\n",
       "<li><p>Kafka also requires ZooKeeper, which is included in the distribution.</p>\n",
       "</li>\n",
       "<li><p>To start ZooKeeper:</p>\n",
       "<pre><code class=\"language-sh\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p>In a new terminal, start Kafka:</p>\n",
       "<pre><code class=\"language-sh\">bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h4>4. Create a Topic (Optional)</h4>\n",
       "<ul>\n",
       "<li>You can create a Kafka topic using:<pre><code class=\"language-sh\">bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h4>5. Produce and Consume Messages (Optional)</h4>\n",
       "<ul>\n",
       "<li>To send messages:<pre><code class=\"language-sh\">bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>To read messages:<pre><code class=\"language-sh\">bin/kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "<h3>Conclusion</h3>\n",
       "<p>This is a basic setup for running Kafka locally. Depending on your use case, you may want to look into more complex configurations or cluster setups.</p>\n",
       "<h3>Clarifying Question</h3>\n",
       "<p>Do you have a specific operating system in mind for the installation, or are you considering running Kafka in a particular environment (like Docker)? Let me know if you want to explore any specific areas or configurations!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = runner.loop(prompt='how do I install kafka', callback=callback) \n",
    "# enjoy better clickable markdown formatting below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1fd1a06-8a87-4622-a6a9-b8c4de3d73d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Great choice! Using Docker simplifies the setup process for Kafka significantly. Hereâ€™s how to set up Kafka using Docker.</p>\n",
       "<h3>Step-by-Step Guide to Install Kafka with Docker</h3>\n",
       "<h4>1. Install Docker</h4>\n",
       "<p>Make sure Docker is installed on your machine. You can follow the instructions on the <a href=\"https://docs.docker.com/get-docker/\">official Docker website</a> to install it for your operating system.</p>\n",
       "<h4>2. Create a <code>docker-compose.yml</code> File</h4>\n",
       "<p>Create a new directory for your Kafka project and create a file named <code>docker-compose.yml</code> with the following content:</p>\n",
       "<pre><code class=\"language-yaml\">version: '3.8'\n",
       "services:\n",
       "  zookeeper:\n",
       "    image: wurstmeister/zookeeper:3.4.6\n",
       "    ports:\n",
       "      - &quot;2181:2181&quot;\n",
       "\n",
       "  kafka:\n",
       "    image: wurstmeister/kafka:latest\n",
       "    ports:\n",
       "      - &quot;9092:9092&quot;\n",
       "    environment:\n",
       "      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9092\n",
       "      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9092\n",
       "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT\n",
       "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
       "    depends_on:\n",
       "      - zookeeper\n",
       "</code></pre>\n",
       "<h4>3. Start Kafka and Zookeeper</h4>\n",
       "<p>Run the following command in the directory where your <code>docker-compose.yml</code> file is located:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose up -d\n",
       "</code></pre>\n",
       "<p>This command will start both Zookeeper and Kafka as background services.</p>\n",
       "<h4>4. Create a Kafka Topic (Optional)</h4>\n",
       "<p>Exec into the Kafka container to create a topic. Run:</p>\n",
       "<pre><code class=\"language-bash\">docker exec -it &lt;kafka_container_name&gt; kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "<p>To find the <code>&lt;kafka_container_name&gt;</code>, you can list running containers:</p>\n",
       "<pre><code class=\"language-bash\">docker ps\n",
       "</code></pre>\n",
       "<h4>5. Produce and Consume Messages (Optional)</h4>\n",
       "<p>To produce messages:</p>\n",
       "<pre><code class=\"language-bash\">docker exec -it &lt;kafka_container_name&gt; kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<p>To consume messages:</p>\n",
       "<pre><code class=\"language-bash\">docker exec -it &lt;kafka_container_name&gt; kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<h3>Stopping the Services</h3>\n",
       "<p>To stop and remove the containers, run:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose down\n",
       "</code></pre>\n",
       "<h3>Conclusion</h3>\n",
       "<p>Following these steps will set up Kafka using Docker easily. If you need to customize the configuration or further explore functionalities like replication, let me know!</p>\n",
       "<h3>Clarifying Question</h3>\n",
       "<p>Do you have any specific configurations or topics you want to create within Kafka, or do you need help with something else related to Docker?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# playing again with RAG LLM call with our search tool use under the hood:\n",
    "# - lets now have a follow up question -\n",
    "# before LLm asked << Would you like to explore more about Kafka topics or configurations?  >>\n",
    "# follow up question is 'I want to use docker'\n",
    "# and we need to provide the history as llm is stateless - << previous_messages=messages >>\n",
    "# thus we save the follow-up answer in a new variable new_messages to keep track of our conversation:\n",
    "\n",
    "new_messages = runner.loop(\n",
    "    prompt='I want to use docker',\n",
    "    previous_messages=messages,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a7b429-d7f1-4f29-8d7b-f34bc3e5fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# finally full search agent implemented with interface\n",
    "# type STOP to exit as usual\n",
    "\n",
    "messages = runner.run();\n",
    "# NB - in Jupyter, a trailing semicolon suppresses the automatic output display of the last expression.\n",
    "# Itâ€™s basically a trick to keep notebooks tidy when you donâ€™t want large outputs printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b71621-555f-4178-a6e2-d68fa4fe0d41",
   "metadata": {},
   "source": [
    "## RAG - LLM tool use - adding more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a329a3-91fc-4817-86db-c196d49d4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will have 2 tools - search and add entry\n",
    "# Alexey Grigoriev usually asks ChatGPT to add doc strings and type hints to functions\n",
    "# thus tools can be created faster:\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def search(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search the FAQ database for entries matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query text to look up in the course FAQ.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "    \"\"\"\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def add_entry(question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new entry to the FAQ database.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be added to the FAQ database.\n",
    "        answer (str): The corresponding answer to the question.\n",
    "    \"\"\"\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef19699e-51e3-4b86-b841-d5912a97eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets actually add it:\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search)\n",
    "agent_tools.add_tool(add_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea083c49-c215-4e8d-8b6b-11ddbc376fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'question parameter'},\n",
       "    'answer': {'type': 'string', 'description': 'answer parameter'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and check that both tools are added:\n",
    "\n",
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20243be1-28ae-45ad-b43c-8be7de5fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our agent loop wrapper \n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0af44222-26b9-4447-9c62-c11fdfe372e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: What is the temperature of boiling water\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"boiling water temperature\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"boiling water temperature\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_ruSusH9fYxdJz9BTXwEozquC', 'output': '[]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"temperature of water boiling\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"temperature of water boiling\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_hGCsPDRPFVYvWLnhZl1j1kuT', 'output': '[]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"boiling point of water\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"boiling point of water\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_ElL7mMQ9zqNkexKfeGq8hzMR', 'output': '[\\n  {\\n    \"text\": \"After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you\\\\u2019ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Homework and Leaderboard - what is the system for points in the course management platform?\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 17\\n  },\\n  {\\n    \"text\": \"This error appeared when running the command: docker build -t taxi_ingest:v001 .\\\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn\\\\u2019t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\\\nsudo chown -R $USER dir_path\\\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\\\n\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\tAdded by\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\tKenan Arslanbay\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Docker - build error: error checking context: \\'can\\'t stat \\'/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data\\'\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 67\\n  },\\n  {\\n    \"text\": \"For the HW1 I encountered this issue. The solution is\\\\nSELECT * FROM zones AS z WHERE z.\\\\\"Zone\\\\\" = \\'Astoria Zone\\';\\\\nI think columns which start with uppercase need to go between \\\\u201cColumn\\\\u201d. I ran into a lot of issues like this and \\\\u201c \\\\u201d made it work out.\\\\nAddition to the above point, for me, there is no \\\\u2018Astoria Zone\\\\u2019, only \\\\u2018Astoria\\\\u2019 is existing in the dataset.\\\\nSELECT * FROM zones AS z WHERE z.\\\\\"Zone\\\\\" = \\'Astoria\\\\u2019;\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"SQL - SELECT * FROM zones_taxi WHERE Zone=\\'Astoria Zone\\'; Error Column Zone doesn\\'t exist\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 154\\n  },\\n  {\\n    \"text\": \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn\\\\u2019t have the necessary permissions to access the specified GCP credentials .json file.\\\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\\\nSolution: Inside the Mage app:\\\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\\\nCopy/paste GCP service account credentials into the .json key file and save\\\\nUpdate code to point to this file. E.g.\\\\nenviron[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json\\'\",\\n    \"section\": \"Module 2: Workflow Orchestration\",\\n    \"question\": \"Mage - ArrowException Cannot open credentials file\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 195\\n  },\\n  {\\n    \"text\": \"could not parse \\'pickup_datetime\\' as timestamp for field pickup_datetime (position 2)\\\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\",\\n    \"section\": \"Module 3: Data Warehousing\",\\n    \"question\": \"GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 211\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems that there isn't a specific entry related to the temperature of boiling water in the FAQ database. However, I can provide the answer based on general knowledge:</p>\n",
       "<p>The boiling point of water is usually 100 degrees Celsius (212 degrees Fahrenheit) at sea level. This temperature can change depending on atmospheric pressure; for example, at higher altitudes, water boils at a lower temperature due to reduced pressure.</p>\n",
       "<p>Would you like to know more about how altitude affects boiling point, or is there anything else specific you'd like to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Add this back to FAQ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"What is the temperature of boiling...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"question\":\"What is the temperature of boiling water?\",\"answer\":\"The boiling point of water is typically 100 degrees Celsius (212 degrees Fahrenheit) at sea level. It can vary depending on atmospheric pressure; for example, at higher altitudes, water boils at a lower temperature due to reduced pressure.\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_w4thQX8qND9OT36a4TNGp6yG', 'output': 'null'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The information about the boiling point of water has been successfully added to the FAQ.</p>\n",
       "<p>If you have any further questions or need clarification on other subjects, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# and run it - type STOP as usual to exit:\n",
    "\n",
    "runner.run();\n",
    "# when we get answer we now can ask LLM to add it back to FAQ\n",
    "# I ask model What is the temperature of boiling water - this is NOT in our FAQ\n",
    "# when it answers I will ask to add it to FAQ using our 'add_entry' tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "023227c2-c9e7-4550-a1ed-46e67379b346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the temperature of boiling water?',\n",
       " 'text': 'The boiling point of water is typically 100 degrees Celsius (212 degrees Fahrenheit) at sea level. It can vary depending on atmospheric pressure; for example, at higher altitudes, water boils at a lower temperature due to reduced pressure.',\n",
       " 'section': 'user added',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You: Add this back to FAQ\n",
    "# Function call: add_entry({\"question\":\"What is the temperature of boiling...)\n",
    "# lets check if it is added there -\n",
    "\n",
    "index.docs[-1]\n",
    "\n",
    "# it prints:\n",
    "# {'question': 'What is the temperature of boiling water?',\n",
    "#  'text': 'The boiling point of water is typically 100 degrees Celsius (212 degrees Fahrenheit) at sea level. It can vary depending on atmospheric pressure; for example, at higher altitudes, water boils at a lower temperature due to reduced pressure.',\n",
    "#  'section': 'user added',\n",
    "#  'course': 'data-engineering-zoomcamp'}\n",
    "\n",
    "\n",
    "# this happens because above we defined\n",
    "# doc = {\n",
    "#         'question': question,\n",
    "#         'text': answer,\n",
    "#         'section': 'user added',\n",
    "#         'course': 'data-engineering-zoomcamp'\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ba319-3f10-4766-a92c-b77483a9d00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

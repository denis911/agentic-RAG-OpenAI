{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff344490-394e-479f-bc97-17882476d056",
   "metadata": {},
   "source": [
    "## STEP 0 - install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3366dc0c-7a3d-4955-9b84-b4d06301c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9206afd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# source is https://www.youtube.com/watch?v=GH3lrOsU3AU\n",
    "# and notebook is https://github.com/DataTalksClub/llm-zoomcamp/blob/main/0a-agents/notebook.ipynb\n",
    "# Follow along this tutorial: https://github.com/alexeygrigorev/rag-agents-workshop\n",
    "\n",
    "# STEP 0 - installing packages we need here and in the VS code terminal:\n",
    "\n",
    "# Run in VS code terminal:\n",
    "# pip install --upgrade pip # no reminders after!\n",
    "# pip install tqdm notebook==7.1.2 openai elasticsearch==8.13.0 pandas scikit-learn ipywidgets\n",
    "# jupyter notebook # to run jupyter engine locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e37acb-1feb-4da5-9ab5-b35d80df1171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install minsearch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e675499-6969-48f7-b42a-3b51a37c3516",
   "metadata": {},
   "source": [
    "## STEP 1 - download evaluation data from Github and build in-memory search index with minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23e682f-5025-448e-9687-28e26d0fce22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       " 'section': 'Module 6: Best practices',\n",
       " 'question': 'How to destroy infrastructure created via GitHub Actions',\n",
       " 'course': 'mlops-zoomcamp'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download our FAQ dataset\n",
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[-1] # to see a format of downloaded FAQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7c1561-c33b-4817-9160-0d659cd8102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x71629c047470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build index - takes a few seconds with minsearch\n",
    "\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d58d90-b092-4318-9daf-5c22ed75bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search function with weights: boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# number of results 5 and course is hard coded here  - filter_dict={'course': 'data-engineering-zoomcamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee44aa1e-eb04-436a-bd70-c944c625b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Can I still join the course?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526dec30-411e-4965-8c95-8fa2f136cb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 2},\n",
       " {'text': \"No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 11},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 7},\n",
       " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 0},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 3}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if our search function works \n",
    "\n",
    "search(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53aa958-7f3d-4366-a8d8-77f5eeb644e1",
   "metadata": {},
   "source": [
    "## STEP 2 - build prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a7ef93-818e-4e91-9f57-52b34c7c6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a prompt\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c19daf-7c13-4a5e-a202-e92fe9871560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "Can I still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "\n",
      "</CONTEXT>\n"
     ]
    }
   ],
   "source": [
    "# test how our prompt build works by calling search(question)\n",
    "\n",
    "prompt = build_prompt(question, search(question))\n",
    "print(prompt) # better formatted for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437dbe4-a21b-4d5b-b38c-fcb1c2a65faa",
   "metadata": {},
   "source": [
    "## STEP 3 - Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "310bba81-83a1-45e9-a3c7-49be5bd834b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to LLM, we entered our API KEY already at the first cell\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8959bb41-fe64-4a0c-a513-09193cc1ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)\n",
    "# just repeating step 1 - keyword search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68040dd-3e23-4a44-8d55-30f651bce87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, search_results)\n",
    "# repeating step 2 - building prompt on top of search results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92bf5fd8-fa18-4c07-b8c3-96eaaada704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to send our prompt to llm \n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cbbc89-489a-439f-886e-8c83fc3e1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course after the start date. Even if you don't register, you are eligible to submit homework. However, keep in mind that there will be deadlines for turning in the final projects, so it's advisable not to leave everything until the last minute.\n"
     ]
    }
   ],
   "source": [
    "# test if our llm can answer something meaningful based on prompt provided\n",
    "\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada0a00b-186c-4ada-b4a8-a1b1732e81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "Can I still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "\n",
      "</CONTEXT>\n"
     ]
    }
   ],
   "source": [
    "print(prompt) # this is what we have sent to llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cb933-452c-41f3-b99c-44c72bc0af59",
   "metadata": {},
   "source": [
    "## STEP 4 - assemble our RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71781bd1-7beb-4ad4-8ab8-5c15ddd7e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our RAG pipeline\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "003417eb-7010-48af-bdbc-02da09f89fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but there is no information available in the FAQ database regarding how to patch KDE under FreeBSD. Please consult the official KDE or FreeBSD documentation or forums for assistance.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test by irrelevant question - something which is NOT in FAQ\n",
    "\n",
    "rag(\"How do I patch KDE under FreeBSD?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1788745b-4d66-4458-9e01-238b33536efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context provided does not include any information regarding Shakespeare or his views on peeling a carrot. Therefore, I cannot answer the question about what Shakespeare said about peeling a carrot.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"What Shakespeare said about peeling a carrot?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "321524b2-58ec-49a0-a6e5-0984cc69dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka in Docker, first ensure that your Kafka broker Docker container is functioning properly. You can check the status of your Docker containers by running the command `docker ps`. If the Kafka broker is not running, navigate to the directory containing your Docker Compose YAML file and execute the command `docker compose up -d` to start all the instances.\n"
     ]
    }
   ],
   "source": [
    "# relevant question - it actually was in FAQ\n",
    "answer = rag(\"How to run Kafka in Docker\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37a06640-c69c-4799-b4e6-8ce236fae194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching KDE under FreeBSD involves several steps, including downloading the source code, applying your patch, and rebuilding the application or library. Below is a general guide on how to do this:\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "1. **Ensure FreeBSD Ports and Source Tree are Installed:**\n",
      "   Make sure you have the FreeBSD ports collection installed. If not, you can install it with the following command:\n",
      "   ```sh\n",
      "   portsnap fetch extract\n",
      "   ```\n",
      "\n",
      "2. **Install Required Tools:**\n",
      "   Make sure you have the necessary development tools and libraries. You can install them using:\n",
      "   ```sh\n",
      "   pkg install git cmake gcc gmake\n",
      "   ```\n",
      "\n",
      "### Steps to Patch KDE\n",
      "\n",
      "1. **Locate the KDE Port:**\n",
      "   Find the KDE port you want to patch. For example, if you want to patch `kde5`, you can look for it in the ports collection, typically under `/usr/ports/x11/kde5`.\n",
      "\n",
      "2. **Navigate to the Port Directory:**\n",
      "   Change to the directory of the specific port. For instance:\n",
      "   ```sh\n",
      "   cd /usr/ports/x11/kde5\n",
      "   ```\n",
      "\n",
      "3. **Fetch the Latest Source Code:**\n",
      "   Update the ports tree and fetch the latest source code:\n",
      "   ```sh\n",
      "   make fetch\n",
      "   ```\n",
      "\n",
      "4. **Create Your Patch:**\n",
      "   Create the patch you want to apply. This can be done using the `diff` command. For example, if you made changes to a file:\n",
      "   ```sh\n",
      "   diff -u original_file.cpp modified_file.cpp > your_patch.patch\n",
      "   ```\n",
      "\n",
      "5. **Apply the Patch:**\n",
      "   Place your patch in the appropriate directory. For KDE ports, this usually goes into the `files` subdirectory of the port:\n",
      "   ```sh\n",
      "   mkdir -p files && mv your_patch.patch files/\n",
      "   ```\n",
      "\n",
      "   In the port's Makefile, you may need to specify the patch using `PATCHFILES`:\n",
      "   ```makefile\n",
      "   PATCHFILES= your_patch.patch\n",
      "   ```\n",
      "\n",
      "6. **Build the Port:**\n",
      "   Once your patch is in place, compile and install the port:\n",
      "   ```sh\n",
      "   make install clean\n",
      "   ```\n",
      "\n",
      "7. **Test Your Changes:**\n",
      "   After installation, test your patched version of KDE to ensure that the changes have been successfully applied and everything works as expected.\n",
      "\n",
      "### Additional Notes\n",
      "\n",
      "- **Reverting a Patch:** If you need to revert a patch, you can comment it out or remove it from the `files` directory, then rebuild the port.\n",
      "- **Diff Options:** You can adjust the `diff` options according to your needs (e.g., `-u` for unified diff).\n",
      "- **Contributing Back:** If you believe your patch may be useful to others, consider contributing it back to the FreeBSD ports collection or KDE community.\n",
      "- **Documentation and Resources:** Refer to the [FreeBSD Porter's Handbook](https://www.freebsd.org/doc/en/books/porters-handbook/) for more detailed information about port creation and management.\n",
      "\n",
      "Following these steps should help you successfully patch KDE applications under FreeBSD. Always ensure you back up your original files before making changes.\n"
     ]
    }
   ],
   "source": [
    "# LLM by itself know the answer, but our RAG is prohibiting it\n",
    "# above rag(\"How do I patch KDE under FreeBSD?\") returned nothing, but - \n",
    "print(llm(\"How do I patch KDE under FreeBSD?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb3baa8-7046-4262-a63e-4b304fe0ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually this was the main idea for STEP 5 below - if llm can answer question by itself, nice!\n",
    "# if not - it should be able to search our FAQ database and build the context for answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f64f5c-3f07-4f42-aac4-ef1aa7b2a98b",
   "metadata": {},
   "source": [
    "## STEP 5 - \"Agentic\" RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869f1f09-7145-49ee-a222-d173457b20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially we only modify our prompt - so llm can decide either to give an answer immediately\n",
    "# or use a SEARCH tool to get more CONTEXT from our FAQ\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75385600-f09c-444a-8189-00d450e3321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Can I still join the course?'\n",
    "context = 'EMPTY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9642963-cdde-472e-b98b-1a134cc4b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "Can I still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2c6d4e-5dfb-473c-a0c5-a655849e90db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"action\": \"SEARCH\",\\n\"reasoning\": \"The question about joining the course requires specific information regarding enrollment deadlines or policies, which is not provided in the current context.\"\\n}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer_json \n",
    "# model decided to use \"SEARCH\" function because CONTEXT is empty\n",
    "# and provided a reason for it  - \"reasoning\": \"I am unsure about the specific enrollment dates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60d4dfe-ad36-4e86-a873-43c123206642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# we can parse the llm answer and use tool if appropriate:\n",
    "\n",
    "answer = json.loads(answer_json)\n",
    "answer['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d71f231-1936-48ec-a271-e77a83823e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets ask LLM something it knows already well:\n",
    "question = 'Can I run Docker on Windows 10?'\n",
    "context = 'EMPTY'\n",
    "prompt = prompt_template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f1e9aa1-c624-4c31-b23a-4796fe243aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"action\": \"ANSWER\",\\n\"answer\": \"Yes, you can run Docker on Windows 10. Docker provides a version called Docker Desktop that works on Windows 10 Pro, Enterprise, and Education editions, and it uses the Windows Subsystem for Linux (WSL 2) for running Linux containers. If you are using Windows 10 Home edition, you can still run Docker Desktop, but it will rely on WSL 2 directly. You will need to ensure that WSL 2 is installed and enabled before installing Docker Desktop.\",\\n\"source\": \"OWN_KNOWLEDGE\"\\n}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer_json \n",
    "# it provides answer immediately - zero shot - action\": \"ANSWER\":\n",
    "# and says it knows it already - \"source\": \"OWN_KNOWLEDGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c445f22-9ca0-4d4d-b5dc-84dfcaaaff42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANSWER'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = json.loads(answer_json)\n",
    "answer['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b107c7e-5b1d-4862-821b-edc3cb9be37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model says SEARCH then we need to use a search tool and build context + update prompt\n",
    "\n",
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    return context.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b6c1333-832a-4378-b322-84c24611982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "Can I still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# asking something it should search for:\n",
    "question = 'Can I still join the course?'\n",
    "\n",
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt) # for better formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6e63838-4743-4629-b5ff-5e20e4b137ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"Yes, you can still join the course even after the start date. While registration is encouraged, you can submit the homework without officially registering. Just keep in mind that there are deadlines for the final projects, so be sure to manage your time wisely.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# lets ask llm after we performed the search as advised and updated llm context\n",
    "\n",
    "answer_json = llm(prompt)\n",
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "653e62b3-403f-4766-a302-32b224cd7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to put everything together and parse our response json automatically\n",
    "# algo - if \"action\": \"SEARCH\" then llm uses our FAQ search tool\n",
    "# if \"action\": \"ANSWER\" - then it answers immediately ...\n",
    "\n",
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b8617e7-653c-4f9c-a952-0e4321bfb2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': 'The question is about how to join the course, and the context is currently empty, so I need to check the FAQ database for guidance on enrollment procedures.'}\n",
      "need to perform search...\n",
      "{'action': 'ANSWER', 'answer': \"To join the course, you need to register before the course starts. Use the provided registration link to sign up. The course will start on 15th January 2024 at 17h00, and you can also find important announcements by joining the course's Telegram channel. Don't forget to subscribe to the course public Google Calendar to keep track of important dates.\", 'source': 'CONTEXT'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, you need to register before the course starts. Use the provided registration link to sign up. The course will start on 15th January 2024 at 17h00, and you can also find important announcements by joining the course's Telegram channel. Don't forget to subscribe to the course public Google Calendar to keep track of important dates.\",\n",
       " 'source': 'CONTEXT'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our agentic RAG function\n",
    "\n",
    "agentic_rag_v1('how do I join the course?')\n",
    "# 'action': 'SEARCH', 'reasoning': 'The context is empty...\n",
    "\n",
    "# 'action': 'ANSWER'\n",
    "# 'source': 'OWN_KNOWLEDGE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b01c8ed2-2b6e-416d-93b7-511a3b2e8d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': 'To patch KDE under FreeBSD, you generally need to follow these steps:\\n\\n1. **Install the necessary tools**: Ensure that you have `portsnap` or `ports` collection installed, along with `editors` or `git` for modifying code.\\n\\n2. **Fetch the KDE source code**: You can download the KDE ports from the FreeBSD ports tree using `portsnap fetch extract` or from the official KDE repositories.\\n\\n3. **Locate the specific port**: Navigate to the directory of the KDE component you wish to patch (e.g., `/usr/ports/x11/kde5`).\\n\\n4. **Create a patch**: Modify the source files as needed and then create a patch file using the `diff` command. Example:\\n   ```\\n   diff -u originalfile.cpp modifiedfile.cpp > mypatch.patch\\n   ```\\n\\n5. **Apply the patch**: Move the patch file to the relevant port directory (e.g., `/usr/ports/x11/kde5/files/`) and apply it using the `patch` command:\\n   ```\\n   patch < mypatch.patch\\n   ```\\n\\n6. **Build and install**: Run `make install` in the port directory to build the patched version of KDE.\\n\\n7. **Test the installation**: Finally, ensure everything works correctly after applying the patch by running KDE and checking for any issues.\\n\\nMake sure to back up any original files and test the patched version thoroughly before using it in a production environment.', 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': 'To patch KDE under FreeBSD, you generally need to follow these steps:\\n\\n1. **Install the necessary tools**: Ensure that you have `portsnap` or `ports` collection installed, along with `editors` or `git` for modifying code.\\n\\n2. **Fetch the KDE source code**: You can download the KDE ports from the FreeBSD ports tree using `portsnap fetch extract` or from the official KDE repositories.\\n\\n3. **Locate the specific port**: Navigate to the directory of the KDE component you wish to patch (e.g., `/usr/ports/x11/kde5`).\\n\\n4. **Create a patch**: Modify the source files as needed and then create a patch file using the `diff` command. Example:\\n   ```\\n   diff -u originalfile.cpp modifiedfile.cpp > mypatch.patch\\n   ```\\n\\n5. **Apply the patch**: Move the patch file to the relevant port directory (e.g., `/usr/ports/x11/kde5/files/`) and apply it using the `patch` command:\\n   ```\\n   patch < mypatch.patch\\n   ```\\n\\n6. **Build and install**: Run `make install` in the port directory to build the patched version of KDE.\\n\\n7. **Test the installation**: Finally, ensure everything works correctly after applying the patch by running KDE and checking for any issues.\\n\\nMake sure to back up any original files and test the patched version thoroughly before using it in a production environment.',\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how patch KDE under FreeBSD?')\n",
    "# 'reasoning': 'The student is asking for a specific method to patch KDE under FreeBSD, and the context is currently empty...\n",
    "\n",
    "# 'action': 'ANSWER'\n",
    "# 'source': 'OWN_KNOWLEDGE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3a7ab-7e06-4d0c-8245-231be569e9a9",
   "metadata": {},
   "source": [
    "## STEP 6 - Agentic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3841e63-0c3d-49c1-b634-7a3aba2ac9ab",
   "metadata": {},
   "source": [
    "Part 2: Agentic search\n",
    "\n",
    "So far we had two actions only: search and answer.\n",
    "\n",
    "But we can let our \"agent\" formulate one or more search queries - and do it for a few iterations until we found an answer\n",
    "\n",
    "Let's build a prompt:\n",
    "\n",
    "List available actions:\n",
    "\n",
    "Search in FAQ\n",
    "\n",
    "-- Answer using own knowledge\n",
    "\n",
    "-- Answer using information extracted from FAQ\n",
    "\n",
    "-- Provide access to the previous actions\n",
    "\n",
    "Have clear stop criteria (no more than X iterations)\n",
    "\n",
    "We also specify the output format, so it's easier to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acbb494e-f04c-481c-ac2d-bc946050b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplication function\n",
    "\n",
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a715a8-07c9-405c-b4d1-371510a09e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0b3478b-3a30-4c24-99f1-588368f33182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetitive task - search, update context and search again if needed + delete duplicate results\n",
    "\n",
    "question = 'how do I do well on module 1'\n",
    "max_iterations = 3\n",
    "iteration_number = 0\n",
    "search_queries = []\n",
    "search_results  = []\n",
    "previous_actions = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e463cfa-2b6d-4259-a5ee-f48d8e97665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I do well on module 1\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a93ee516-d289-4869-b6d2-8d84dc6e4579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"action\": \"SEARCH\",\\n\"reasoning\": \"To provide a comprehensive answer on how to do well in module 1, I need to find specific strategies, tips, or resources outlined in the FAQ that address academic success in the module.\",\\n\"keywords\": [\"how to succeed in module 1\", \"tips for module 1\", \"successful strategies module 1\"]\\n}'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5c688b-4670-4674-a10e-bd0a75e305e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'SEARCH',\n",
       " 'reasoning': 'To provide a comprehensive answer on how to do well in module 1, I need to find specific strategies, tips, or resources outlined in the FAQ that address academic success in the module.',\n",
       " 'keywords': ['how to succeed in module 1',\n",
       "  'tips for module 1',\n",
       "  'successful strategies module 1']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = json.loads(answer_json)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c1f14b8-9df2-455e-b279-06d2aa186aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to succeed in module 1',\n",
       " 'tips for module 1',\n",
       " 'successful strategies module 1']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions.append(answer)\n",
    "keywords = answer['keywords']\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee55f140-1237-4ac8-9e7d-ddbcbc2de2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to search each keyword and add results to a context\n",
    "\n",
    "for kw in keywords:\n",
    "    search_queries.append(kw)\n",
    "    sr = search(kw)\n",
    "    search_results.extend(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42fa5520-8ac5-4717-8c38-4e6c218adb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=‚Äù${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}‚Äù\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‚Äòpy4j‚Äô of the spark you‚Äôre using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 323},\n",
       " {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 299},\n",
       " {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Module Not Found Error in Jupyter Notebook .',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 322},\n",
       " {'text': \"Issue:\\ne‚Ä¶\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the ‚Äú ModuleNotFoundError: No module named 'psycopg2' ‚Äú error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 112},\n",
       " {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 124},\n",
       " {'text': \"Error raised during the jupyter notebook‚Äôs cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module ‚Äúpsycopg2‚Äù. Can be installed by Conda or pip.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 125}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see what we found\n",
    "\n",
    "search_results = dedup(search_results)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6026702-4a47-43c1-a90a-3bc38e7438d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_number = 2\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36622fa1-aacf-4f4a-8ff7-2ef3a817a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': \"Since the previous search did not yield specific tips or strategies for succeeding in Module 1, I'll search for generic study tips or strategies that may apply to success in an academic module, focusing on relevant skills such as Docker and Terraform.\", 'keywords': ['study tips for Docker', 'success strategies for Terraform', 'best practices for learning Docker and Terraform', 'effective studying techniques for Module 1']}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "141d51c4-98ad-4238-8988-1d127880c5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a detailed answer on how to be successful in module 1, I need to find specific strategies, tips, or requirements related to that module from the FAQ database.\",\n",
      "  \"keywords\": [\n",
      "    \"successful module 1\",\n",
      "    \"module 1 study tips\",\n",
      "    \"module 1 requirements\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "successful module 1\n",
      "module 1 study tips\n",
      "module 1 requirements\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=‚Äù${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}‚Äù\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‚Äòpy4j‚Äô of the spark you‚Äôre using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e‚Ä¶\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the ‚Äú ModuleNotFoundError: No module named 'psycopg2' ‚Äú error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module ‚Äúkafka‚Äù not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a detailed answer on how to be successful in module 1, I need to find specific strategies, tips, or requirements related to that module from the FAQ database.\", \"keywords\": [\"successful module 1\", \"module 1 study tips\", \"module 1 requirements\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer to the question about being successful in Module 1, I need to gather information specifically focused on study strategies, best practices, and requirements for that module from the FAQ database.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 study strategies\",\n",
      "    \"Module 1 requirements\",\n",
      "    \"how to succeed in Module 1\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "successful module 1\n",
      "module 1 study tips\n",
      "Module 1 study strategies\n",
      "Module 1 requirements\n",
      "module 1 requirements\n",
      "how to succeed in Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=‚Äù${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}‚Äù\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‚Äòpy4j‚Äô of the spark you‚Äôre using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e‚Ä¶\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the ‚Äú ModuleNotFoundError: No module named 'psycopg2' ‚Äú error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module ‚Äúkafka‚Äù not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a detailed answer on how to be successful in module 1, I need to find specific strategies, tips, or requirements related to that module from the FAQ database.\", \"keywords\": [\"successful module 1\", \"module 1 study tips\", \"module 1 requirements\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer to the question about being successful in Module 1, I need to gather information specifically focused on study strategies, best practices, and requirements for that module from the FAQ database.\", \"keywords\": [\"Module 1 study strategies\", \"Module 1 requirements\", \"how to succeed in Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1, which covers Docker and Terraform, consider the following strategies:\\n\\n1. **Understand the Basics**: Gain a solid understanding of Docker and Terraform concepts, including containers, images, orchestration, and infrastructure as code.\\n\\n2. **Hands-On Practice**: Engage in hands-on practice by setting up Docker containers and using Terraform to manage infrastructure. Follow tutorial guides and execute example projects.\\n\\n3. **Learn Command Line Proficiency**: Familiarize yourself with command line operations for both Docker and Terraform, as much of the interaction with these tools occurs via the terminal.\\n\\n4. **Study the Documentation**: Regularly reference the official documentation for Docker and Terraform. This will provide you with the most accurate and up-to-date information.\\n\\n5. **Utilize Online Resources**: Explore additional online tutorials, videos, and forums. Community discussions can provide valuable insights and tips.\\n\\n6. **Group Study**: If possible, form study groups with peers. This promotes knowledge sharing and helps clarify doubts.\\n\\n7. **Focus on Installing Requirements**: Ensure you have all the necessary tools and software installed correctly; this includes Docker Desktop, Terraform binary, and any relevant extensions or plugins.\\n\\n8. **Debugging and Problem Solving**: When encountering issues, utilize debugging best practices to troubleshoot problems systematically. Use forums like Stack Overflow for solutions to common issues.\\n\\nBy applying these methods and maintaining consistent study habits, you'll enhance your effectiveness and understanding in this module.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# now lets create a function which will perform a search in a loop\n",
    "\n",
    "question = \"what do I need to do to be successful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b370b760-00b6-4a9f-a871-6b2534e796a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be successful in Module 1, which covers Docker and Terraform, consider the following strategies:\n",
      "\n",
      "1. **Understand the Basics**: Gain a solid understanding of Docker and Terraform concepts, including containers, images, orchestration, and infrastructure as code.\n",
      "\n",
      "2. **Hands-On Practice**: Engage in hands-on practice by setting up Docker containers and using Terraform to manage infrastructure. Follow tutorial guides and execute example projects.\n",
      "\n",
      "3. **Learn Command Line Proficiency**: Familiarize yourself with command line operations for both Docker and Terraform, as much of the interaction with these tools occurs via the terminal.\n",
      "\n",
      "4. **Study the Documentation**: Regularly reference the official documentation for Docker and Terraform. This will provide you with the most accurate and up-to-date information.\n",
      "\n",
      "5. **Utilize Online Resources**: Explore additional online tutorials, videos, and forums. Community discussions can provide valuable insights and tips.\n",
      "\n",
      "6. **Group Study**: If possible, form study groups with peers. This promotes knowledge sharing and helps clarify doubts.\n",
      "\n",
      "7. **Focus on Installing Requirements**: Ensure you have all the necessary tools and software installed correctly; this includes Docker Desktop, Terraform binary, and any relevant extensions or plugins.\n",
      "\n",
      "8. **Debugging and Problem Solving**: When encountering issues, utilize debugging best practices to troubleshoot problems systematically. Use forums like Stack Overflow for solutions to common issues.\n",
      "\n",
      "By applying these methods and maintaining consistent study habits, you'll enhance your effectiveness and understanding in this module.\n"
     ]
    }
   ],
   "source": [
    "print(answer['answer']) # for better look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62c2b829-d498-4755-bfda-321fbeffc9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6371ca73-3c02-48cb-8717-9222c2e09465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together agentic search as a function\n",
    "\n",
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3,\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        print()\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10863975-d00a-417b-afd1-a980c0dbb66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I need to gather specific advice on how to prepare for the course to provide a comprehensive answer to the student.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation tips\",\n",
      "    \"how to prepare for class\",\n",
      "    \"study resources for course\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "study resources for course\n",
      "how to prepare for class\n",
      "course preparation tips\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool ‚ÄúX‚Äù instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can‚Äôt support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors‚Äô code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: iPython - Pandas parsing dates with ‚Äòread_csv‚Äô\n",
      "answer: Pandas can interpret ‚Äústring‚Äù column values as ‚Äúdatetime‚Äù directly when reading the CSV file using ‚Äúpd.read_csv‚Äù using the parameter ‚Äúparse_dates‚Äù, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\n",
      "pandas.read_csv ‚Äî pandas 2.1.4 documentation (pydata.org)\n",
      "Example from week 1\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "'yellow_tripdata_2021-01.csv',\n",
      "nrows=100,\n",
      "parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
      "df.info()\n",
      "which will output\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 18 columns):\n",
      "#   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      "0   VendorID               100 non-null    int64\n",
      "1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n",
      "2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n",
      "3   passenger_count        100 non-null    int64\n",
      "4   trip_distance          100 non-null    float64\n",
      "5   RatecodeID             100 non-null    int64\n",
      "6   store_and_fwd_flag     100 non-null    object\n",
      "7   PULocationID           100 non-null    int64\n",
      "8   DOLocationID           100 non-null    int64\n",
      "9   payment_type           100 non-null    int64\n",
      "10  fare_amount            100 non-null    float64\n",
      "11  extra                  100 non-null    float64\n",
      "12  mta_tax                100 non-null    float64\n",
      "13  tip_amount             100 non-null    float64\n",
      "14  tolls_amount           100 non-null    float64\n",
      "15  improvement_surcharge  100 non-null    float64\n",
      "16  total_amount           100 non-null    float64\n",
      "17  congestion_surcharge   100 non-null    float64\n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 14.2+ KB\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark-shell: unable to load native-hadoop library for platform - Windows\n",
      "answer: If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\n",
      "java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\n",
      "module @0x3c947bc5\n",
      "Solution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Standalone Mode on Windows\n",
      "answer: Open a CMD terminal in administrator mode\n",
      "cd %SPARK_HOME%\n",
      "Start a master node: bin\\spark-class org.apache.spark.deploy.master.Master\n",
      "Start a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\n",
      "bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\n",
      "spark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\n",
      "Use --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\n",
      "Now you can access Spark UI through localhost:8080\n",
      "Homework for Module 5:\n",
      "Do not refer to the homework file located under /05-batch/code/. The correct file is located under\n",
      "https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: TypeError when using spark.createDataFrame function on a pandas df\n",
      "answer: Error:\n",
      "spark.createDataFrame(df_pandas).schema\n",
      "TypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\n",
      "Solution:\n",
      "Affiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don‚Äôt have to take out any data from your dataset. Something like this can help:\n",
      "df = spark.read \\\n",
      ".options(\n",
      "header = \"true\", \\\n",
      "inferSchema = \"true\", \\\n",
      ") \\\n",
      ".csv('path/to/your/csv/file/')\n",
      "Solution B:\n",
      "It's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n",
      "# Only take rows that have no null values\n",
      "pandas_df= pandas_df[pandas_df.notnull().all(1)]\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: How to spark standalone cluster is run on windows OS\n",
      "answer: Change the working directory to the spark directory:\n",
      "if you have setup up your SPARK_HOME variable, use the following;\n",
      "cd %SPARK_HOME%\n",
      "if not, use the following;\n",
      "cd <path to spark installation>\n",
      "Creating a Local Spark Cluster\n",
      "To start Spark Master:\n",
      "bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
      "Starting up a cluster:\n",
      "bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: General course-related questions\n",
      "question: Any books or additional resources you recommend?\n",
      "answer: Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather specific advice on how to prepare for the course to provide a comprehensive answer to the student.\", \"keywords\": [\"course preparation tips\", \"how to prepare for class\", \"study resources for course\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"Since I am looking for specific preparation tips and study resources that could help students get ready for the course, I'll look for information regarding study techniques, recommended materials, and strategies that could aid in preparing for the course effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"how to prepare for a data engineering course\",\n",
      "    \"study techniques for data engineering\",\n",
      "    \"recommended resources for data engineering course\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "study resources for course\n",
      "recommended resources for data engineering course\n",
      "study techniques for data engineering\n",
      "how to prepare for a data engineering course\n",
      "how to prepare for class\n",
      "course preparation tips\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a ‚Äúlive‚Äù cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool ‚ÄúX‚Äù instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can‚Äôt support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors‚Äô code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: iPython - Pandas parsing dates with ‚Äòread_csv‚Äô\n",
      "answer: Pandas can interpret ‚Äústring‚Äù column values as ‚Äúdatetime‚Äù directly when reading the CSV file using ‚Äúpd.read_csv‚Äù using the parameter ‚Äúparse_dates‚Äù, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\n",
      "pandas.read_csv ‚Äî pandas 2.1.4 documentation (pydata.org)\n",
      "Example from week 1\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "'yellow_tripdata_2021-01.csv',\n",
      "nrows=100,\n",
      "parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
      "df.info()\n",
      "which will output\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 18 columns):\n",
      "#   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      "0   VendorID               100 non-null    int64\n",
      "1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n",
      "2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n",
      "3   passenger_count        100 non-null    int64\n",
      "4   trip_distance          100 non-null    float64\n",
      "5   RatecodeID             100 non-null    int64\n",
      "6   store_and_fwd_flag     100 non-null    object\n",
      "7   PULocationID           100 non-null    int64\n",
      "8   DOLocationID           100 non-null    int64\n",
      "9   payment_type           100 non-null    int64\n",
      "10  fare_amount            100 non-null    float64\n",
      "11  extra                  100 non-null    float64\n",
      "12  mta_tax                100 non-null    float64\n",
      "13  tip_amount             100 non-null    float64\n",
      "14  tolls_amount           100 non-null    float64\n",
      "15  improvement_surcharge  100 non-null    float64\n",
      "16  total_amount           100 non-null    float64\n",
      "17  congestion_surcharge   100 non-null    float64\n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 14.2+ KB\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark-shell: unable to load native-hadoop library for platform - Windows\n",
      "answer: If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\n",
      "java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\n",
      "module @0x3c947bc5\n",
      "Solution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Standalone Mode on Windows\n",
      "answer: Open a CMD terminal in administrator mode\n",
      "cd %SPARK_HOME%\n",
      "Start a master node: bin\\spark-class org.apache.spark.deploy.master.Master\n",
      "Start a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\n",
      "bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\n",
      "spark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\n",
      "Use --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\n",
      "Now you can access Spark UI through localhost:8080\n",
      "Homework for Module 5:\n",
      "Do not refer to the homework file located under /05-batch/code/. The correct file is located under\n",
      "https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: TypeError when using spark.createDataFrame function on a pandas df\n",
      "answer: Error:\n",
      "spark.createDataFrame(df_pandas).schema\n",
      "TypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\n",
      "Solution:\n",
      "Affiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don‚Äôt have to take out any data from your dataset. Something like this can help:\n",
      "df = spark.read \\\n",
      ".options(\n",
      "header = \"true\", \\\n",
      "inferSchema = \"true\", \\\n",
      ") \\\n",
      ".csv('path/to/your/csv/file/')\n",
      "Solution B:\n",
      "It's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n",
      "# Only take rows that have no null values\n",
      "pandas_df= pandas_df[pandas_df.notnull().all(1)]\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: How to spark standalone cluster is run on windows OS\n",
      "answer: Change the working directory to the spark directory:\n",
      "if you have setup up your SPARK_HOME variable, use the following;\n",
      "cd %SPARK_HOME%\n",
      "if not, use the following;\n",
      "cd <path to spark installation>\n",
      "Creating a Local Spark Cluster\n",
      "To start Spark Master:\n",
      "bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
      "Starting up a cluster:\n",
      "bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: General course-related questions\n",
      "question: Any books or additional resources you recommend?\n",
      "answer: Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?\n",
      "answer: Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\n",
      "Move the contents of the .gitignore file in your main .gitignore.\n",
      "Use the terminal to cd into the Mage folder and:\n",
      "run ‚Äúgit remote remove origin‚Äù to de-couple the Mage repo,\n",
      "run ‚Äúrm -rf .git‚Äù to delete local git files,\n",
      "run ‚Äúgit add .‚Äù to add the current folder as changes to stage, commit and push.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\n",
      "answer: This error appeared when running the command: docker build -t taxi_ingest:v001 .\n",
      "When feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn‚Äôt access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\n",
      "Since at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\n",
      "A more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\n",
      "You can fix the problem by changing the permission of the directory on ubuntu with following command:\n",
      "sudo chown -R $USER dir_path\n",
      "On windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\tAdded by\n",
      "\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Which playlist on YouTube should I refer to?\n",
      "answer: All the main videos are stored in the Main ‚ÄúDATA ENGINEERING‚Äù playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\n",
      "Below is the MAIN PLAYLIST‚Äô. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\n",
      "h\n",
      "ttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: Hack to load yellow and green trip data for 2019 and 2020\n",
      "answer: I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main ¬∑ DataTalksClub/data-engineering-bootcamp (github.com)\n",
      "But it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\n",
      "Then I found another hack shared in the slack which was suggested by Victoria.\n",
      "[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\n",
      "Please watch until the end as there is few schema changes required to be done\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather specific advice on how to prepare for the course to provide a comprehensive answer to the student.\", \"keywords\": [\"course preparation tips\", \"how to prepare for class\", \"study resources for course\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"Since I am looking for specific preparation tips and study resources that could help students get ready for the course, I'll look for information regarding study techniques, recommended materials, and strategies that could aid in preparing for the course effectively.\", \"keywords\": [\"how to prepare for a data engineering course\", \"study techniques for data engineering\", \"recommended resources for data engineering course\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To prepare for the course, make sure to register before the course starts using the provided registration link. Join the public Google Calendar to stay updated on class times and events, and subscribe to the course's Telegram channel for announcements. It's also advisable to check the materials available after the course concludes, as you can continue learning at your own pace and work on your final capstone project. Additionally, visit the document containing recommended resources for further reading: [Awesome Data Engineering](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md).\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# and test our agentic search function\n",
    "\n",
    "answer = agentic_search('how do I prepare for the course?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eefe355a-e1eb-473c-a94e-de6f191bc4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER_CONTEXT', 'answer': \"To prepare for the course, make sure to register before the course starts using the provided registration link. Join the public Google Calendar to stay updated on class times and events, and subscribe to the course's Telegram channel for announcements. It's also advisable to check the materials available after the course concludes, as you can continue learning at your own pace and work on your final capstone project. Additionally, visit the document containing recommended resources for further reading: [Awesome Data Engineering](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md).\", 'source': 'CONTEXT'}\n"
     ]
    }
   ],
   "source": [
    "# for better formatting\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5dff8c5-7f8d-48b2-978e-98cb00be9a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for the course, make sure to register before the course starts using the provided registration link. Join the public Google Calendar to stay updated on class times and events, and subscribe to the course's Telegram channel for announcements. It's also advisable to check the materials available after the course concludes, as you can continue learning at your own pace and work on your final capstone project. Additionally, visit the document containing recommended resources for further reading: [Awesome Data Engineering](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md).\n"
     ]
    }
   ],
   "source": [
    "# no need of parsing json?\n",
    "# answer = json.loads(answer)\n",
    "\n",
    "print(answer['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7da6d-c060-41ac-ac61-f53c6bcccef9",
   "metadata": {},
   "source": [
    "## STEP 7 - Function calling (\"tool use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876c3d7-f38e-48ab-adf0-5d6307001090",
   "metadata": {},
   "source": [
    "Part 3: Function calling - code from\n",
    "https://github.com/alexeygrigorev/rag-agents-workshop/tree/main\n",
    "\n",
    "Function calling in OpenAI\n",
    "\n",
    "We put all this logic inside our prompt.\n",
    "\n",
    "But OpenAI and other providers provide a convenient API for adding extra functionality like search.\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "It's called \"function calling\" - you define functions that the model can call, and if it decides to make a call, it returns structured output for that.\n",
    "\n",
    "For example, let's take our search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a38727b-9713-453f-a5da-5cd071fef0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search tool\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a953f43-55fc-4aa3-9ea4-747b9c7e2325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module ‚Äúkafka‚Äù not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp', '_id': 372}, {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n‚Äî--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp', '_id': 425}, {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal', 'course': 'data-engineering-zoomcamp', '_id': 389}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp', '_id': 379}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‚Äògradle shadowjar‚Äô, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp', '_id': 387}]\n"
     ]
    }
   ],
   "source": [
    "answer = search(\"How to run Kafka in Docker\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4868ba76-c685-4003-bfa5-6ac3207940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to describe our search function for OpenAI / Pydantic:\n",
    "# (this is a json wrapper to use our minsearch function = search tool from above ))\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# \"name\": \"search\" - based on this Python will call search(query) from above\n",
    "# and pass a property \"query\": { \"type\": \"string\" ... when / into this function call object is created?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af8d97c9-d74f-4c07-b0dc-57b081370ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_2dd1C8lgE68HKjnlGgCyh9bK', name='search', type='function_call', id='fc_68bc63d64e50819eabd5f8a08add38780388b2f6a5ecf56d', status='completed')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have:\n",
    "\n",
    "# name: search\n",
    "# description: when to use it\n",
    "# parameters: all the arguments that the function can take and their description\n",
    "# In order to use function calling, we'll use a newer API - the \"responses\" API (not \"chat completions\" as previously):\n",
    "\n",
    "question = \"How do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "response.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5844bbe7-f856-4290-ae6c-20c72ced21c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_2dd1C8lgE68HKjnlGgCyh9bK', name='search', type='function_call', id='fc_68bc63d64e50819eabd5f8a08add38780388b2f6a5ecf56d', status='completed')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make a call to search - to the earlier minsearch << def search(query) >> function:\n",
    "# to the index object which is <minsearch.append.AppendableIndex at 0x7473af0c0cb0> \n",
    "\n",
    "calls = response.output\n",
    "call = calls[0]\n",
    "call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ee7e628-ed6f-4ff8-b7d0-0c4c930dd8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call_2dd1C8lgE68HKjnlGgCyh9bK'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_id = call.call_id\n",
    "call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3322b8d-c26d-4c78-bffb-6e8fb634b47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_name = call.name\n",
    "f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73a9f755-eb72-45fc-bcb0-14c997569cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to do well in module 1'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments = json.loads(call.arguments)\n",
    "arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8e1558c-b31f-4852-848d-9d9abac56aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nice Python trick - use globals() function which knows all variables and functions in our enviromnent\n",
    "\n",
    "globals()[f_name] # which is f_name = call.name\n",
    "# <function __main__.search(query)> - this is our minsearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9fe77e32-c28f-482c-b9aa-e87e51b6b741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'search',\n",
       " 'description': 'Search the FAQ database',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'type': 'string',\n",
       "    'description': 'Search query text to look up in the course FAQ.'}},\n",
       "  'required': ['query'],\n",
       "  'additionalProperties': False}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[\"search_tool\"] \n",
    "# prints our dictionary we use to talk to openAI function calling interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83b7123c-d8bc-4783-a11a-2ffc8af3b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to do well in module 1'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essentially using globals() we can call any function and pass arguments\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbf377e4-83df-42ed-a385-57a6ffe57763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
      "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
      "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 299\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 322\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 323\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 112\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 125\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "f = globals()[f_name]\n",
    "\n",
    "results = f(**arguments) # here we call our minsearch search function to find 5 'module 1 tips' \n",
    "\n",
    "search_results = json.dumps(results, indent=2)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aa45990b-7b54-4076-8f7e-b7e23ba2f3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_2dd1C8lgE68HKjnlGgCyh9bK', name='search', type='function_call', id='fc_68bc63d64e50819eabd5f8a08add38780388b2f6a5ecf56d', status='completed')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57a2f938-72ab-4f9f-9b75-8190a51df4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'developer',\n",
       "  'content': \"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\"},\n",
       " {'role': 'user', 'content': 'How do I do well in module 1?'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_messages # this is our memory ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06bf7cb1-2858-4182-99a5-746d4d7dad6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'developer',\n",
       "  'content': \"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\"},\n",
       " {'role': 'user', 'content': 'How do I do well in module 1?'},\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_2dd1C8lgE68HKjnlGgCyh9bK', name='search', type='function_call', id='fc_68bc63d64e50819eabd5f8a08add38780388b2f6a5ecf56d', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_2dd1C8lgE68HKjnlGgCyh9bK',\n",
       "  'output': '[\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 112\\n  },\\n  {\\n    \"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 125\\n  }\\n]'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  save both the response and the result of the function call\n",
    "\n",
    "chat_messages.append(call)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": search_results,\n",
    "})\n",
    "\n",
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d74282f-b5d3-4d1c-bd8d-94641e65bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now chat_messages contains both the call description (so it keeps track of history) and the results\n",
    "\n",
    "# Let's make another call to the model:\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1817c4f8-dce2-4622-a263-56756f54bb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1, consider the following tips:\n",
      "\n",
      "1. **Understand Key Concepts**: Familiarize yourself with the basics of Docker and Terraform, as these are central to the module. Make sure you understand how they work and their applications in data engineering.\n",
      "\n",
      "2. **Follow Installation Instructions**: Ensure that you correctly install all necessary tools and libraries. For instance, the `psycopg2` library is crucial for PostgreSQL integration. If you encounter errors such as `ModuleNotFoundError: No module named 'psycopg2'`, use the command:\n",
      "   ```bash\n",
      "   pip install psycopg2-binary\n",
      "   ```\n",
      "\n",
      "3. **Hands-On Practice**: Engage in hands-on exercises and projects using Docker and Terraform. Setting up a PostgreSQL container and connecting to it through Python can solidify your understanding.\n",
      "\n",
      "4. **Troubleshooting**: Familiarize yourself with common errors and their solutions. For example, if you encounter problems with SQLAlchemy and PostgreSQL connection, ensure that the `psycopg2` module is installed.\n",
      "\n",
      "5. **Consult Resources**: Utilize course materials, forums, and FAQs to troubleshoot issues you might face. There are often solutions available from past students who faced similar challenges.\n",
      "\n",
      "6. **Ask Questions**: Don‚Äôt hesitate to seek help from instructors or peers if you're stuck on a concept. Engaging in discussions can often clarify confusing topics.\n",
      "\n",
      "Following these strategies will enhance your understanding and performance in Module 1. Good luck!\n"
     ]
    }
   ],
   "source": [
    "# This time it should be the response (but also can be another call):\n",
    "\n",
    "r = response.output[0]\n",
    "print(r.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3422a-bf57-4454-8a2e-c710c882d166",
   "metadata": {},
   "source": [
    "## STEP 8 - Making multiple calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b016350-bf61-4639-83f2-93718e0214e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I do well in module 1?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making multiple calls\n",
    "# What if we want to make multiple calls? Change the developer prompt a little:\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "question # 'How do I do well in module 1?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19b669a7-b92b-438b-af14-d95c1ef4fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organising code to look neat\n",
    "# First, create a function do_call:\n",
    "\n",
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": tool_call_response.call_id,\n",
    "        \"output\": json.dumps(result, indent=2),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "198523ca-9058-4511-b311-b0b375a19cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call\n",
      "function_call\n"
     ]
    }
   ],
   "source": [
    "# Now iterate over responses:\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4196bea0-ab71-486c-b2c9-8895daf9b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "\n",
      "To do well in Module 1, here are some tips and important details that can help you succeed:\n",
      "\n",
      "1. **Familiarize with Tools**:\n",
      "   - Make sure you understand how to set up Docker and Terraform as they are crucial for this module.\n",
      "   - Follow the official documentation and the readme files closely. For example, ensure that you have Docker installed and configured correctly.\n",
      "\n",
      "2. **Resolve Common Issues**:\n",
      "   - You might encounter specific errors like `ModuleNotFoundError: No module named 'psycopg2'`. To resolve this, you can run:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary\n",
      "     ```\n",
      "     If issues persist, consider updating your package manager and reinstalling.\n",
      "\n",
      "3. **Practice Coding**:\n",
      "   - Regularly practice coding examples and assignments provided in the module to solidify your understanding.\n",
      "   - Use Jupyter Notebook effectively, and ensure all necessary packages are installed.\n",
      "\n",
      "4. **Engage with the Community**:\n",
      "   - Don‚Äôt hesitate to ask questions in forums or community boards if you face challenges.\n",
      "\n",
      "5. **Stay Organized**:\n",
      "   - Keep your repositories and code organized for easier reference and debugging.\n",
      "\n",
      "6. **Check for Compatibility**:\n",
      "   - Ensure that you have compatible versions of libraries that are required for your projects, especially for SQLAlchemy and PostgreSQL setups. \n",
      "\n",
      "7. **Utilize Resources**:\n",
      "   - Make use of all course materials provided, including lectures, recorded sessions, and additional readings.\n",
      "\n",
      "8. **Stay Consistent**:\n",
      "   - Dedicate regular time to study and practice, and set specific goals for each week.\n",
      "\n",
      "By following these guidelines, you will enhance your chances for success in Module 1. If you have more specific questions or areas you're struggling with, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# First call will probably be function call, so let's do another one:\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "    print()\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.content[0].text) \n",
    "\n",
    "# this time it is a text response - as LLm has now context in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb5736-9891-453c-baee-12168de222ac",
   "metadata": {},
   "source": [
    "## STEP 9 - a Python class to decorate the chatting logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05a093a4-234e-4eca-b9c2-e8fffb2f728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-06 17:31:38--  https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3495 (3.4K) [text/plain]\n",
      "Saving to: ‚Äòchat_assistant.py‚Äô\n",
      "\n",
      "chat_assistant.py   100%[===================>]   3.41K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-09-06 17:31:38 (57.4 MB/s) - ‚Äòchat_assistant.py‚Äô saved [3495/3495]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py\n",
    "\n",
    "# code from https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py\n",
    "# or https://github.com/alexeygrigorev/rag-agents-workshop/blob/main/chat_assistant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b93ea6-cab3-4eac-b0f2-b831368b1f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
